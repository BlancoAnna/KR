{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79E5BMffKNq7"
      },
      "source": [
        "# Seminar 1 - A Song of Graphs and Search\n",
        "\n",
        "---\n",
        "\n",
        "**Course**: Graphs and Network Analysis\n",
        "\n",
        "**Degree**: Artificial Intelligence Degree (UAB)\n",
        "\n",
        "**Topic**: Practical seminar that includes exercises from units 1 to 6\n",
        "\n",
        "**Activity description**: Most of us are familiar with the Game of Thrones books or series. For those who do not know it, it is a fictional series from the HBO chain, inspired by the series of novels \"A Song of Ice and Fire\", which tells the experiences of a group of characters from different noble houses on the fictional continent of *Westeros* to have control of the Iron Throne and rule the seven kingdoms that make up the territory. The series' success has spawned many blogs and other sources about the series, with additional resources. The graphs that we propose to use in this exercise represent the characters of the series (or books) as nodes, and their co-appearance in a scene (the weights of the edges are higher if two characters appear simultaneously more times). So we have a social network of characters. We will use these graphs to work on some of the concepts seen in the first units of the course (graph and node metrics, search and routes). Finally, we will generate synthetic graphs that simulate a realistic network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLiz4rCDKvnz"
      },
      "source": [
        "## Qualification\n",
        "\n",
        "**Submission**: An '.ipynb' file from the colab corresponding to each group will be delivered (this very same file, adding the code blocks and explanations that correspond to each activity). To get the file you will need to go to File --> Download. Remember that you will have to answer and analyze the different problems. Coding alone will NOT be evaluated: explaining and reasoning about the solution of the problem is essential. **You should provide explanations of the obtained results for *at least* the exercises marked with the ðŸ’¬ symbol**.\n",
        "The outcome of this seminar will thus be an analysis of the network at different levels: global metrics, node importance, shortest paths, random graphs, and visualization.\n",
        "\n",
        "**Delivery form**: The work must be done in **groups of two people** and delivered through the virtual campus (in the section corresponding to Seminar 1). Only one group member needs to submit.\n",
        "\n",
        "**Doubts**: For any questions, apart from class sessions, you can contact ivan.erill@uab.cat.\n",
        "\n",
        "**Deadline**: March 13th 23:59 CET (all day).\n",
        "\n",
        "**Grade**: The grade of the seminars (seminar 1 + seminar 2) has a weight of 10% on the final grade of the course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mN24cx9gHve"
      },
      "source": [
        "# Authors\n",
        "\n",
        "**Lab group:** GrupLab-10\n",
        "\n",
        "**Student 1 - X**\n",
        "\n",
        "**Student 2 - Y **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qbjyP1Dies"
      },
      "source": [
        "## 1. Environment setup\n",
        "----\n",
        "\n",
        "The main libraries that will be used in this seminar are the following:\n",
        "\n",
        "* [NetworkX](https://networkx.github.io/)\n",
        "* [Pandas](https://pandas.pydata.org/)\n",
        "* [Matplotlib](https://matplotlib.org/)\n",
        "* [NumPy](https://numpy.org/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PWbwZx7z0Ek"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scipy networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSnP2jzpDiB4"
      },
      "outputs": [],
      "source": [
        "!apt install libgraphviz-dev\n",
        "!pip install pygraphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai3-8Rz2zAFD"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jPVT43xAiSm"
      },
      "source": [
        "## 2. Data collection\n",
        "\n",
        "---\n",
        "\n",
        "This seminar is based on data from *Game of Thrones* and \"A Song of Ice and Fire\" curated by Andrew Beveridge. Data is available from two different github repositories:\n",
        "\n",
        "* [Book to Network](https://github.com/mathbeveridge/asoiaf)\n",
        "* [Script to Network](https://github.com/mathbeveridge/gameofthrones)\n",
        "\n",
        "In each of them, there is a *data* folder with several *.csv* files that encode nodes and edges of different networks.\n",
        "\n",
        "To download the data in the *colab* environment you can run the following command:\n",
        "\n",
        "```\n",
        "$ !wget https://raw.githubusercontent.com/mathbeveridge/repo_name/master/data/file_id-nodes.csv\n",
        "$ !wget https://raw.githubusercontent.com/mathbeveridge/repo_name/master/data/file_id-edges.csv\n",
        "```\n",
        "\n",
        "\n",
        "where,\n",
        "\n",
        "* **repo_name** is the name of the repository, *asoiaf* for the Books and *gameofthrones* for the Script.\n",
        "* **file_id** is the ID of the file you can find with the link. This indicates the book or season number.\n",
        "\n",
        "For example, to download the graph of the first season of the series, we would run:\n",
        "\n",
        "```\n",
        "$ !wget https://raw.githubusercontent.com/mathbeveridge/gameofthrones/master/data/got-s1-nodes.csv\n",
        "$ !wget https://raw.githubusercontent.com/mathbeveridge/gameofthrones/master/data/got-s1-edges.csv\n",
        "```\n",
        "\n",
        "The downloaded files can be found in */content/file_name*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7FtY-xO2vfv"
      },
      "source": [
        "For this activity, we will work with the graph generated from **all the _books_**\n",
        "\n",
        "\n",
        "*  **Download only the two .csv files corresponding to the graph generated from all the books (asoiaf-all)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBFCzKx6AmOq"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mathbeveridge/asoiaf/master/data/asoiaf-all-edges.csv\n",
        "!wget https://raw.githubusercontent.com/mathbeveridge/asoiaf/master/data/asoiaf-all-nodes.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twfpe2fpAptq"
      },
      "source": [
        "## 3. Data load\n",
        "\n",
        "---\n",
        "\n",
        "The function *csv_to_graph()* creates a NetworkX graph from the *.csv* files encoding edges and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd2_o4UZAqN7"
      },
      "outputs": [],
      "source": [
        "def csv_to_graph(file_id_nodes: str, file_id_edges: str, origin: str = 'book') \\\n",
        "                    -> nx.graph:\n",
        "    \"\"\"Return a nx.graph\n",
        "\n",
        "    Build a graph given a csv file for nodes and edge.\n",
        "    origin controls the source of the graph to adapt the node features.\n",
        "    \"\"\"\n",
        "\n",
        "    if origin == 'book':\n",
        "        key1, key2 = 'weight', 'book'\n",
        "    elif origin == 'script':\n",
        "        key1, key2 = 'Weight', 'Season'\n",
        "    else:\n",
        "        raise NameError('Unknown origin {}'.format(origin))\n",
        "\n",
        "    nodes = pd.read_csv(file_id_nodes)\n",
        "    edges = pd.read_csv(file_id_edges)\n",
        "\n",
        "    if key2 not in edges:\n",
        "        key2 = 'id'\n",
        "\n",
        "    g = nx.Graph()\n",
        "    for row in nodes.iterrows():\n",
        "        g.add_node(row[1]['Id'], name=row[1]['Label'])\n",
        "\n",
        "    for row in edges.iterrows():\n",
        "        g.add_edge(row[1]['Source'],row[1]['Target'],\n",
        "                   weight=1/row[1][key1], id=row[1][key2])\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXfsNMBoBA0u"
      },
      "source": [
        "* **Create a NetworkX graph from the downloaded files using the `csv_to_graph` function.** [Optionally, you can repeat the process with the graph generated from the series]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bkvDRy4BG_-"
      },
      "outputs": [],
      "source": [
        "g_book = csv_to_graph('asoiaf-all-nodes.csv', 'asoiaf-all-edges.csv', origin='book')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYA9HhHfTF5J"
      },
      "source": [
        "* **Generate a first exploratory visualization of the graph.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Zft9qsFpM3"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Choose layout\n",
        "pos = nx.spring_layout(g_book, seed=42)\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(g_book, pos, node_size=100, node_color='cyan', edgecolors='black')\n",
        "\n",
        "# Draw edges with transparency\n",
        "nx.draw_networkx_edges(g_book, pos, alpha=0.5, width=1.2, edge_color='gray')\n",
        "\n",
        "# Show the graph\n",
        "plt.title(\"Graph Visualization\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3NMU6sFTWah"
      },
      "source": [
        "## 4. General graph metrics\n",
        "---\n",
        "\n",
        "Perform a general summary of the Network properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5QMkkdTcez"
      },
      "source": [
        "* **ðŸ’¬  Obtain the order, size and density of the graph, as well as the average degree of its nodes.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhMIYrihTfGz"
      },
      "outputs": [],
      "source": [
        "# Order\n",
        "print('Order:', g_book.order())\n",
        "# Size\n",
        "print('Size:', g_book.size())\n",
        "# Density\n",
        "print('Density:', nx.density(g_book))\n",
        "# Average degree of the nodes\n",
        "average_degree = sum(dict(g_book.degree()).values()) / g_book.number_of_nodes()\n",
        "print(\"Average degree:\", average_degree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIqk5NY-e97f"
      },
      "source": [
        "ðŸ’¬ : The fact that the order of the graph is 796 means that there are 796 nodes in it. In our particular case, this implies that in the graph there are represented **796 characters** of Game of Thrones. <br>\n",
        "<br>\n",
        "The size (2823) refers to the number of **connections** (edges) in the graph. This means that in our graph there are 2823 connections between all the people. <br>\n",
        "<br>\n",
        "The density measures how connected the graph is. A density of 0.0089 means that only 0.89% of all possible edges in a fully connected graph are present. This indicates that our graph is very sparse or that most nodes are not directly connected to each other. <br>\n",
        "<br>\n",
        "Finally, the average degree is of 7.09, meaning that each node on average has **7 connections**. While the graph is sparse overall, individual nodes still have a moderate number of connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UNE33UKTeJb"
      },
      "source": [
        "* **Check that it is a connected undirected graph.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ac20vgWUM8S"
      },
      "outputs": [],
      "source": [
        "# Check that the graph is connected\n",
        "if not nx.is_directed(g_book):\n",
        "    print(\"The graph is undirected.\")\n",
        "else:\n",
        "    print(\"The graph is directed.\")\n",
        "\n",
        "# Check that the graph is undirected\n",
        "if nx.is_connected(g_book):\n",
        "    print(\"The graph is connected.\")\n",
        "else:\n",
        "    print(\"The graph is not connected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4a8idcD49W_"
      },
      "source": [
        "* **ðŸ’¬ Make a small report on the metrics of the given graph (diameter, radius, average network distance, average clustering coefficient).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpQpgHRIth7N"
      },
      "outputs": [],
      "source": [
        "# Diameter\n",
        "print('Diameter:', nx.diameter(g_book))\n",
        "# Radius\n",
        "print('Radius:', nx.radius(g_book))\n",
        "# Average network distance\n",
        "print('Average network distance:', nx.average_shortest_path_length(g_book))\n",
        "# Clustering coefficient\n",
        "print('Clustering coefficient: ', nx.average_clustering(g_book))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAgSjt-dfE_P"
      },
      "source": [
        "ðŸ’¬ : The diameter of a graph is the longest shortest path between any two nodes. In our case, the diameter of our graph is 9, meaning that the two most distant nodes in the graph are 9 steps apart when following the shortest possible paths. This suggests that the graph has a relatively small structure, where even the most separated nodes are not too distant. This means that even the most distant characters are connected through a maximum of 9 other characters. (*Example: A minor character from Dorne might be linked to someone from the Nightâ€™s Watch through several intermediaries*)<br>\n",
        "<br>\n",
        "A radius of 5 suggests that there exists at least one character who can reach any other character within 5 steps. This implies that some characters are highly central in the story and act as bridges between different groups. (*Examples of this characters could be main characters like Tyrion, Jon Snow, Daenerys, or Varys*) <br>\n",
        "<br>\n",
        "An average shortest path of 3.4 means that most characters are only about 3 or 4 connections apart. This confirms that we have a small-world structure (as we saw with the diameter). Minor characters can be connected with main characters such as Jon Snow or Daenerys by just 3 or 4 nodes. <br>\n",
        "<br>\n",
        "A clustering coefficient of 0.49 indicates that nearly half of a characterâ€™s direct connections also know each other. This suggests the presence of distinct communities within the graph, which aligns with the story, where characters from Kingâ€™s Landing primarily interact within their own circle, while Daenerysâ€™ followers form a separate, closely connected group..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMiA4hFQU7hX"
      },
      "source": [
        "## 5. Centrality metrics: Characters' importance\n",
        "---\n",
        "\n",
        "\n",
        "In this section, we will study the importance of the characters according to their centrality in the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U12IpA9BU_TD"
      },
      "source": [
        "* **Compute the 10 most central nodes in the network taking into account the different types of centrality (degree, betweenness, closeness and eigenvector centrality). Use also PageRank to assess importance of the characters.**\n",
        "\n",
        "  * *centrality_bar_plot()*: Given the corresponding centrality draw a bar graph.\n",
        "  * ðŸ’¬ Try to reason about the changes that you observe with the different types of centrality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0_uGmLkVCms"
      },
      "outputs": [],
      "source": [
        "#centrality is a dictionary generated by networkx centrality functions\n",
        "#with keys=node_ids, values=centrality_values\n",
        "def centrality_bar_plot(centrality, name='betweenness', n=10):\n",
        "\n",
        "    # Extract the top N nodes and sort by centrality value\n",
        "    top_n = dict(sorted(centrality.items(), key=lambda item: item[1], reverse=True)[:n])\n",
        "\n",
        "    values = list(top_n.values()) # Nodes' names array\n",
        "    label = list(top_n.keys()) # Centrality values array\n",
        "\n",
        "    df = pd.DataFrame({'Name': label, name: values})\n",
        "    ax = df.plot.bar(x='Name', y=name, color = 'skyblue', rot=90)\n",
        "\n",
        "    # Adjust figure size for better clarity\n",
        "    plt.title(f\"Top {n} Nodes by {name.capitalize()} Centrality\")\n",
        "    plt.xlabel(\"Character\")\n",
        "    plt.ylabel(\"Centrality Score\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI9WHb_pVG3i"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [10, 4]\n",
        "\n",
        "degree_centrality = nx.degree_centrality(g_book) # Degree Centrality\n",
        "betweenness_centrality = nx.betweenness_centrality(g_book) # Betweenness Centrality\n",
        "closeness_centrality = nx.closeness_centrality(g_book) # Closeness Centrality\n",
        "eigen_centrality = nx.eigenvector_centrality(g_book) # Eigenvalue Centrality\n",
        "\n",
        "centrality_bar_plot(degree_centrality, name='degree')\n",
        "centrality_bar_plot(betweenness_centrality, name='betweenness')\n",
        "centrality_bar_plot(closeness_centrality, name='closeness')\n",
        "centrality_bar_plot(eigen_centrality, name='eigen')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 12]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tyrion Lannister has the highest degree, closeness, and eigenvector centrality, while Jon Snow has the highest betweenness centrality. The fact that the most central characters change depending on the metric makes sense when we look at what each measure represents. <br>\n",
        "\n",
        "- Degree Centrality: Tyrion has a high degree centrality because he directly interacts with many other characters.<br>\n",
        "- Closeness Centrality: Since he is well-connected, he can reach other characters quickly, making him highly accessible within the network.<br>\n",
        "- Eigenvector Centrality: Not only does Tyrion have many connections, but his connections are also influential (e.g., Tywin, Daenerys), making him even more central.<br>\n",
        "- Betweenness Centrality: Jon Snow ranks highest here because he often serves as a bridge between different character groups. This means he connects factions that might not interact otherwise, playing a key role in uniting different parts of the network. <br>\n",
        "\n",
        "\n",
        "Each metric represents a different type of importance, which is why the most central characters vary depending on the measure used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "239DC_LZbRgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koaKpy3NPlrg"
      },
      "outputs": [],
      "source": [
        "# Page rank: [you can use alpha=0.85]\n",
        "pagerank_centrality = nx.pagerank(g_book, alpha=0.85)  # Add PageRank for additional insight\n",
        "centrality_bar_plot(pagerank_centrality, name='pagerank')  # PageRank plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkufO5EfHMb"
      },
      "source": [
        "ðŸ’¬ : Jon Snow also has the highest PageRank, meaning he is one of the most influential characters, as he is connected to other important ones, even if he doesnâ€™t have the most direct connections. This is because PageRank measures importance based on connections to other important nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzdW06TWLpgE"
      },
      "source": [
        "* **What is the subgraph generated by the best connected characters?**\n",
        "  * Use closeness centrality to generate the graph of the 25 most central characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP0PDNFROlzA"
      },
      "outputs": [],
      "source": [
        "#centrality is a dictionary generated by networkx centrality functions\n",
        "#with keys=node_ids, values=centrality_values\n",
        "def centrality_subgraph(g, centrality, name='closeness', n=25):\n",
        "    # Get the top N nodes based on centrality\n",
        "    top_nodes = sorted(centrality, key=centrality.get, reverse=True)[:n]\n",
        "\n",
        "    # Create the subgraph with only these nodes and their connections\n",
        "    subgraph = g.subgraph(top_nodes)\n",
        "\n",
        "    # Draw the subgraph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pos = nx.spring_layout(subgraph, seed=42)  # Layout for better visualization\n",
        "    nx.draw(subgraph, pos, with_labels=True, node_size=700, node_color='cyan', edge_color='gray', font_size=8)\n",
        "\n",
        "    # Title\n",
        "    plt.title(f\"Top {n} {name.capitalize()} Centrality Characters\", fontsize=12)\n",
        "    plt.show()\n",
        "    return subgraph  # Return subgraph for further analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQI3uYUVQC7K"
      },
      "outputs": [],
      "source": [
        "g_subgraph = centrality_subgraph(g_book, nx.closeness_centrality(g_book))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QAANoAzQkIg"
      },
      "source": [
        "* **Draw this subgraph where the nodes are of size proportional to their centrality. Highlight the most central and the least central node in the graph (for instance, use the color of the node to highlight it).**\n",
        "  * Use *closeness centrality* and scale it appropriately to emphasize the importance of different nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrmPPrnLQg-_"
      },
      "outputs": [],
      "source": [
        "# Function to visualize the subgraph\n",
        "def centrality_subgraph_highlighted(g, centrality, name='closeness', n=25):\n",
        "    # Get the top N nodes based on centrality\n",
        "    top_nodes = sorted(centrality, key=centrality.get, reverse=True)[:n]\n",
        "\n",
        "    # Create the subgraph with only these nodes and their connections\n",
        "    subgraph = g.subgraph(top_nodes)\n",
        "\n",
        "    # Scale node size based on centrality\n",
        "    node_sizes = np.array([centrality[node] for node in subgraph.nodes()]) * 1000\n",
        "\n",
        "    # Identify most and least central nodes\n",
        "    max_centrality_node = max(subgraph.nodes, key=lambda node: centrality[node])\n",
        "    min_centrality_node = min(subgraph.nodes, key=lambda node: centrality[node])\n",
        "\n",
        "    # Node colors: highlight most and least central nodes\n",
        "    node_colors = [\"red\" if node == max_centrality_node else\n",
        "                   \"blue\" if node == min_centrality_node else\n",
        "                   \"gray\" for node in subgraph.nodes()]\n",
        "\n",
        "    # Draw the subgraph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pos = nx.spring_layout(subgraph, seed=42)  # Layout for better visualization\n",
        "    nx.draw(subgraph, pos, with_labels=True, node_size=node_sizes, node_color=node_colors, edge_color=\"gray\", font_size=8)\n",
        "\n",
        "    plt.scatter([], [], c=\"red\", label=\"Most Central Node\", s=200)\n",
        "    plt.scatter([], [], c=\"blue\", label=\"Least Central Node\", s=200)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.title(f\"Top {n} {name.capitalize()} Centrality Subgraph\", fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    return subgraph  # Return subgraph for further analysis\n",
        "\n",
        "# Full graph\n",
        "closeness_centrality = nx.closeness_centrality(g_book)\n",
        "\n",
        "# Subgraph\n",
        "closeness_centrality_sub = nx.closeness_centrality(g_subgraph)\n",
        "\n",
        "# Visualize\n",
        "centrality_subgraph_highlighted(g_subgraph, closeness_centrality_sub, name='closeness', n=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrcyWVDeSbKm"
      },
      "source": [
        "* **Draw the tree that the BFS and DFS algorithm would generate to traverse the graph starting from the least central node of the network according to *closeness centrality*.**\n",
        "  * Use *closeness centrality* and scale it appropriately to emphasize the importance of different nodes.\n",
        "  * To get the positions of the nodes, you can use the `graphviz_layout(tree, prog='dot')` command.\n",
        "  * ðŸ’¬ Comment on the obtained result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWqBU-pgGEN1"
      },
      "outputs": [],
      "source": [
        "# Full graph\n",
        "closeness_centrality = nx.closeness_centrality(g_book)\n",
        "\n",
        "# Find the least central node\n",
        "least_central_node = min(closeness_centrality, key=closeness_centrality.get)\n",
        "\n",
        "# BFS\n",
        "bfs_tree = nx.bfs_tree(g_book, source=least_central_node)\n",
        "\n",
        "# Get node sizes based on centrality\n",
        "node_sizes = np.array([closeness_centrality.get(node, 0) for node in bfs_tree.nodes()]) * 1000\n",
        "\n",
        "# Plot BFS tree\n",
        "plt.figure(figsize=(12, 8))  # Increased figure size\n",
        "pos_bfs = graphviz_layout(bfs_tree, prog='dot')  # Hierarchical layout\n",
        "nx.draw(bfs_tree, pos_bfs, with_labels=True, node_size=node_sizes, node_color=\"lightblue\",\n",
        "        edge_color=\"gray\", font_size=6, width=0.8, alpha=0.8)\n",
        "plt.title(f\"BFS Tree Traversal from {least_central_node}\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# DFS\n",
        "dfs_tree = nx.dfs_tree(g_book, source=least_central_node)\n",
        "\n",
        "# Get node sizes based on centrality\n",
        "node_sizes = np.array([closeness_centrality.get(node, 0) for node in dfs_tree.nodes()]) * 1000\n",
        "\n",
        "# Plot DFS tree\n",
        "plt.figure(figsize=(12, 8))  # Increased figure size\n",
        "pos_dfs = graphviz_layout(dfs_tree, prog='dot')  # Hierarchical layout\n",
        "nx.draw(dfs_tree, pos_dfs, with_labels=True, node_size=node_sizes, node_color=\"lightblue\",\n",
        "        edge_color=\"gray\", font_size=6, width=0.8, alpha=0.8)\n",
        "plt.title(f\"DFS Tree Traversal from {least_central_node}\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvC9X71WfJPN"
      },
      "source": [
        "ðŸ’¬ : The BFS and DFS traversals reveal different structures in the Game of Thrones character network, but due to the large dataset, both visualizations end up somewhat messy and overlapping.\n",
        "\n",
        "The **BFS tree** forms a **clear, layered hierarchy**, expanding from Gormon Tyrell in levels. This makes it easy to see how different characters are grouped and connected, reflecting the broader structure of alliances and relationships in Westeros. It visually organizes the network into well-defined clusters, showing how information or influence might spread.\n",
        "\n",
        "In contrast, the **DFS tree** follows a **long, branching path**, diving deep into one connection before backtracking. This results in a more stretched-out structure, reflecting individual storylines that focus on personal interactions rather than overall connectivity. It captures the depth of character relationships but is less organized compared to BFS.\n",
        "\n",
        "Both methods offer different ways to understand the network. BFS gives a **big-picture view**, while DFS emphasizes **individual paths and depth**, much like how character arcs develop in the series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPw518z4bm6X"
      },
      "source": [
        "* **ðŸ’¬ Compute the shortest path between the least and the most central nodes in the complete graph.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the most and least central nodes\n",
        "most_central_node = max(closeness_centrality, key=closeness_centrality.get)\n",
        "least_central_node = min(closeness_centrality, key=closeness_centrality.get)\n",
        "\n",
        "# Compute the shortest path\n",
        "shortest_path = nx.shortest_path(g_book, source=least_central_node, target=most_central_node)\n",
        "\n",
        "# Compute the shortest path length\n",
        "shortest_path_length = nx.shortest_path_length(g_book, source=least_central_node, target=most_central_node)\n",
        "\n",
        "# Print results\n",
        "print(f\"Most central node: {most_central_node}\")\n",
        "print(f\"Least central node: {least_central_node}\")\n",
        "print(f\"Shortest path between them: {shortest_path}\")\n",
        "print(f\"Shortest path length: {shortest_path_length}\")"
      ],
      "metadata": {
        "id": "_x1QzuMFDRGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P64h9BCGfKQ5"
      },
      "source": [
        "ðŸ’¬ : In both the **full graph** and the **subgraph**, **Tyrion Lannister** stays the most central node, but the least central node changes. In the subgraph (top 25 most central nodes), it was **Benjen Stark**, but in the full graph, it turned out to be **Gormon Tyrell**. This makes sense because centrality depends on the whole network structure, and when more characters are included, the rankings shift.\n",
        "\n",
        "The **shortest path** between them goes through six steps: Gormon-Tyrell â†’ Walgrave â†’ Pate-(novice) â†’ Alleras â†’ Aemon-Targaryen-(Maester-Aemon) â†’ Alliser-Thorne â†’ Tyrion-Lannister. That means Gormon is pretty far from Tyrion in terms of connections, needing several intermediaries to reach him.\n",
        "\n",
        "A **shortest path length of 6** shows that Gormon Tyrell is in a very **disconnected part of the network**. He doesnâ€™t have a direct line to Tyrion, which is why he has the lowest closeness centrality. Most of his connections seem to be **through more obscure or less influential characters**, like Walgrave and Pate, rather than major political figures. This means heâ€™s not well integrated into the main hubs of the network, making it harder for him to reach key players like Tyrion quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMmjlE6PLbNa"
      },
      "source": [
        "## 6. Random graph models\n",
        "----\n",
        "Up to this point, we have worked with a graph generated from the data extracted from the *Song of Ice and Fire* books. In the real world, however, obtaining the data needed to construct this graph can become very complex and expensive. This is one of the reasons why, over time, the synthetic generation of graphs has been studied.\n",
        "\n",
        "In this section we will work on the different models described in class. We will generate random graphs and study their properties.\n",
        "\n",
        "* **Generate random graphs with the Uniform, Gilbert and BarabÃ¡si-Albert models. Fix the number of nodes to the order of the GoT graph. Adjust the rest of the parameters of the graph generation function to obtain graphs with similar number of edges.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR-_Q4tIaiDP"
      },
      "source": [
        "### ErdÃ¶s-RÃ©ny: Uniform Model (gnm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMzeVY2GOnl1"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "g_uniform = nx.gnm_random_graph(n=796, m=2823, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jx8XqxAWOk6"
      },
      "source": [
        "### ErdÃ¶s-RÃ©ny: Gilbert Model (gnp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KanjwpLRL3k"
      },
      "outputs": [],
      "source": [
        "# Set the number of nodes and target edges\n",
        "n = 796\n",
        "target_edges = 2823\n",
        "\n",
        "# Estimate probability\n",
        "p = (2 * target_edges) / (n * (n - 1))\n",
        "\n",
        "# Generate graph\n",
        "g_gilbert = nx.gnp_random_graph(n=n, p=p, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWTrYow_WQR3"
      },
      "source": [
        "### BarabÃ¡si-Albert Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSw72kqvRYzx"
      },
      "outputs": [],
      "source": [
        "# Set the number of nodes (n) and target edges\n",
        "n = 796\n",
        "target_edges = 2823\n",
        "\n",
        "# Estimate m based on the formula\n",
        "m = target_edges // (n - 1)\n",
        "\n",
        "# Generate the graph\n",
        "g_barbasi = nx.barabasi_albert_graph(n=n, m=m, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lECOPqH-bku6"
      },
      "outputs": [],
      "source": [
        "g_dict = {'Book': g_book, 'Uniform': g_uniform, 'Erdos': g_gilbert, 'Barbasi': g_barbasi}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkksiBi8MYII"
      },
      "source": [
        "* **ðŸ’¬ Show the order and size of the graph as well as the average degree and clustering coefficient of its nodes. Compute also the intervals between the maximum and minimum centralities for each family of synthetic graphs. Make a small report of the main metrics. Which random graph resembles more closely the graph from the books?**\n",
        "     * You can set the graph generation using a random seed. This way, two different runs will generate exactly the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax0Z-_edMgwW"
      },
      "outputs": [],
      "source": [
        "for k, g in g_dict.items():\n",
        "    # Compute centralities\n",
        "    degree_centrality = nx.degree_centrality(g) # Degree Centrality\n",
        "    betweenness_centrality = nx.betweenness_centrality(g) # Betweenness Centrality\n",
        "    closeness_centrality = nx.closeness_centrality(g) # Closeness Centrality\n",
        "    eigen_centrality = nx.eigenvector_centrality(g) # Eigenvalue Centrality\n",
        "\n",
        "    # Calculate the intervals (max - min)\n",
        "    degree_interval = max(degree_centrality.values()) - min(degree_centrality.values())\n",
        "    betweenness_interval = max(betweenness_centrality.values()) - min(betweenness_centrality.values())\n",
        "    closeness_interval = max(closeness_centrality.values()) - min(closeness_centrality.values())\n",
        "    eigen_interval = max(eigen_centrality.values()) - min(eigen_centrality.values())\n",
        "\n",
        "    print(f\"{k}: \\n\")\n",
        "    print(f\"Order = {g.order()}\")\n",
        "    print(f\"Size = {g.size()}\")\n",
        "    print(f\"Average degree = {sum(dict(g.degree()).values()) / g.number_of_nodes()}\")\n",
        "    print(f\"Clustering coefficient = {nx.average_clustering(g)}\")\n",
        "    print(f\"Degree centrality interval = {degree_interval}\")\n",
        "    print(f\"Betweenness centrality interval = {betweenness_interval}\")\n",
        "    print(f\"Closeness centrality interval = {closeness_interval}\")\n",
        "    print(f\"Eigenvector centrality interval = {eigen_interval}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esuCGxsRfMkc"
      },
      "source": [
        "ðŸ’¬ : All synthetic graphs have the same number of nodes as the Book graph (796). However, the BarabÃ¡si-Albert graph has fewer edges (2379), making it slightly smaller in terms of connections.<br>\n",
        "\n",
        "The BarabÃ¡si-Albert model has the lowest average degree (5.98), which is smaller than the Book's average degree (7.09). The ErdÅ‘sâ€“RÃ©nyi and Uniform models have average degrees around 7, closely matching the Book.<br>\n",
        "\n",
        "The Book's graph has a high clustering coefficient (0.49), meaning its nodes form cohesive clusters. In comparison, the synthetic graphs, especially the Uniform and ErdÅ‘sâ€“RÃ©nyi models, show very low clustering (0.01 to 0.007), indicating less cohesive clusters. The BarabÃ¡si-Albert model, with a clustering coefficient of 0.039, is closer to the Book's graph but still not a perfect match.<br>\n",
        "\n",
        "The centrality intervals of the Bookâ€™s graph are relatively large, particularly for closeness (0.326) and betweenness (0.192) centrality, suggesting greater variation in node importance. Among the synthetic graphs, the BarabÃ¡si-Albert model has the largest centrality intervals, especially for betweenness (0.156) and eigenvector centrality (0.316). The Uniform and ErdÅ‘sâ€“RÃ©nyi models have smaller intervals, indicating more evenly distributed centrality.<br>\n",
        "\n",
        "Overall, the BarabÃ¡si-Albert model most closely resembles the Book's graph, with higher clustering and larger centrality intervals. It also exhibits a more hierarchical structure, similar to the Bookâ€™s network. While not a perfect match, the BarabÃ¡si-Albert model is the closest in terms of connectivity and centrality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEt8rlpHUtoA"
      },
      "source": [
        "* **ðŸ’¬ Check whether the networks (the three randomly generated ones and the network extracted from the books) follow a Power Law.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB5cRKMtUqYQ"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [13, 5]\n",
        "\n",
        "for k, g in g_dict.items():\n",
        "\n",
        "    num_nodes = len(g.nodes())\n",
        "    degrees = dict(g.degree())\n",
        "\n",
        "    # Extract the list of degrees\n",
        "    degree_list = list(degrees.values())\n",
        "\n",
        "    # Create the scatter plot for node degrees\n",
        "    plt.figure()\n",
        "    plt.title(f\"Degree Distribution of {k}\")\n",
        "    plt.xlabel(\"Node Degree\", fontsize=10)\n",
        "    plt.ylabel(\"Number of Nodes\", fontsize=10)\n",
        "\n",
        "    for degree in degree_list:\n",
        "        plt.scatter(degree, degree_list.count(degree), label=f'{k} Graph', alpha=0.6)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKXBK09dfNfr"
      },
      "source": [
        "ðŸ’¬ : Both the network extracted from the book and the BarabÃ¡si-Albert (BA) model follow a **power law distribution**, while the Uniform and Gilbert models follow a distribution closer to **normal**. <br>\n",
        "\n",
        "The reason for this difference lies in the mechansims used to construct them. The BarabÃ¡si-Albert model uses **preferential attachment**, meaning that new nodes are more likely to connect with existing nodes that already have many connections. This creates **large hubsâ€”nodes** with many connections, leading to a power law distribution. In contrast, the ErdÅ‘sâ€“RÃ©nyi (ER) and Gilbert models treat all nodes equally, with no preference for high-degree nodes. As a result, all nodes tend to have a similar number of connections, which results in a more uniform or normal distribution. <br>\n",
        "\n",
        "Real-world networks, like social networks, often have a few highly connected nodes (hubs) that are central to the structure of the network. This organic growth and concentration of connections is captured by the BarabÃ¡si-Albert model, but not by the Uniform or Gilbert models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
